{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6dd15c8e",
   "metadata": {},
   "source": [
    "# SageMaker Training Compiler - Finding Max Batch Size for Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db1cb84",
   "metadata": {},
   "source": [
    "1. [Introduction](#Introduction)  \n",
    "2. [Development Environment and Permissions](#Development-Environment-and-Permissions)\n",
    "    1. [Installation](#Installation)  \n",
    "    2. [Development environment](#Development-environment)  \n",
    "3. [Finding max batch size](#Finding-max-batch-size)\n",
    "    1. [Model and instance type specifications](#Model-and-instance-type-specifications)  \n",
    "    2. [Finding max batch size for SageMaker Training Compiler with Hugging Face and TensorFlow](#Finding-max-batch-size-for-SageMaker-Training-Compiler-with-Hugging-Face-and-TensorFlow)\n",
    "    3. [Wait for find max batch job to complete](#Wait-for-find-max-batch-job-to-complete)\n",
    "4. [Results](#Results)  \n",
    "    1. [Load logs for find max batch job](#Load-logs-for-find-max-batch-job)  \n",
    "5. [Clean up](#Clean-up) \n",
    "6. [Conclusion](#Conclusion) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c125ff",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The SageMaker Training Compiler allows AWS customers to train deep learning models faster on scalable GPU instances managed by SageMaker. The memory optimizations made by SageMaker Training Compiler typically allow for your training job to fit more data into GPU memory. By increasing the batch size as much as possible in your training job, you can speed up your training jobs even further.\n",
    "\n",
    "For example, for a TensorFlow fine-tuning job (Sequence_Len=512, Automatic Mixed Precision (AMP)) with a GPT-2 model from Hugging Face, the maximum batch size that can fit on a ml.p3.2xlarge instance increased from 6 to 20 with the Training Compiler enabled. A list of model examples and maximum batch sizes is available in the SageMaker Training Compiler documentation under \"Tested Models\": https://docs.aws.amazon.com/sagemaker/latest/dg/training-compiler-support.html\n",
    "\n",
    "The goal of this Notebook is to give you an example of how you can find the max batch size for a particular model and instance type. We show you how to find the max batch size for a gpt2 model below running on a `ml.p3.8xlarge` instance. This takes about 1 hour. You can customize this Notebook to fit your use case, and use the resulting max batch size as the value of your batch size parameter in your full training job.\n",
    "\n",
    "The Notebook uses the Hugging Face training scripts (`run_mlm.py` and `run_clm.py`) and a hands-on script (`find_max_batch_size.py`) to iteratively search for the maximum batch for a given GPU instance. \n",
    "\n",
    "This Notebook runs the `run_clm.py` by default, as will be shown in the following sections. If you want to test with your own training script, you need to update the following:\n",
    "- The `find_max_batch_size.py` script - In line 23 to 28 of the script, specify the right directory path and the file name of your training script.\n",
    "- `hyperparameters` - In the following Tune a Native TensorFlow Training Job section, modify the hyperparameters that your training script requires accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094e33ba",
   "metadata": {},
   "source": [
    "## Development Environment and Permissions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b4801c",
   "metadata": {},
   "source": [
    "### Installation\n",
    "\n",
    "This example notebook requires the **SageMaker Python SDK v2.87.0** and **transformers v4.17.0**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "465feea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting sagemaker==2.87.0\n",
      "  Downloading sagemaker-2.87.0.tar.gz (522 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m522.9/522.9 KB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting attrs==20.3.0\n",
      "  Downloading attrs-20.3.0-py2.py3-none-any.whl (49 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 KB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting boto3>=1.20.21\n",
      "  Downloading boto3-1.22.1-py3-none-any.whl (132 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.5/132.5 KB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting google-pasta\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 KB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting numpy>=1.9.0\n",
      "  Downloading numpy-1.22.3-cp39-cp39-macosx_10_14_x86_64.whl (17.6 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting protobuf>=3.1\n",
      "  Downloading protobuf-3.20.1-cp39-cp39-macosx_10_9_x86_64.whl (962 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m962.4/962.4 KB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting protobuf3-to-dict>=0.1.5\n",
      "  Downloading protobuf3-to-dict-0.1.5.tar.gz (3.5 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting smdebug_rulesconfig==1.0.1\n",
      "  Downloading smdebug_rulesconfig-1.0.1-py2.py3-none-any.whl (20 kB)\n",
      "Collecting importlib-metadata>=1.4.0\n",
      "  Downloading importlib_metadata-4.11.3-py3-none-any.whl (18 kB)\n",
      "Collecting packaging>=20.0\n",
      "  Downloading packaging-21.3-py3-none-any.whl (40 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 KB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pandas\n",
      "  Downloading pandas-1.4.2-cp39-cp39-macosx_10_9_x86_64.whl (11.1 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.1/11.1 MB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hCollecting pathos\n",
      "  Downloading pathos-0.2.8-py2.py3-none-any.whl (81 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.7/81.7 KB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n",
      "  Downloading jmespath-1.0.0-py3-none-any.whl (23 kB)\n",
      "Collecting s3transfer<0.6.0,>=0.5.0\n",
      "  Downloading s3transfer-0.5.2-py3-none-any.whl (79 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 KB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting botocore<1.26.0,>=1.25.1\n",
      "  Downloading botocore-1.25.1-py3-none-any.whl (8.7 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting zipp>=0.5\n",
      "  Downloading zipp-3.8.0-py3-none-any.whl (5.4 kB)\n",
      "Collecting pyparsing!=3.0.5,>=2.0.2\n",
      "  Downloading pyparsing-3.0.8-py3-none-any.whl (98 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.5/98.5 KB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting six\n",
      "  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting python-dateutil>=2.8.1\n",
      "  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.7/247.7 KB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pytz>=2020.1\n",
      "  Downloading pytz-2022.1-py2.py3-none-any.whl (503 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m503.5/503.5 KB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting pox>=0.3.0\n",
      "  Downloading pox-0.3.0-py2.py3-none-any.whl (30 kB)\n",
      "Collecting ppft>=1.6.6.4\n",
      "  Downloading ppft-1.6.6.4-py3-none-any.whl (65 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.1/65.1 KB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting multiprocess>=0.70.12\n",
      "  Downloading multiprocess-0.70.12.2-py39-none-any.whl (128 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.7/128.7 KB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting dill>=0.3.4\n",
      "  Downloading dill-0.3.4-py2.py3-none-any.whl (86 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.9/86.9 KB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting urllib3<1.27,>=1.25.4\n",
      "  Downloading urllib3-1.26.9-py2.py3-none-any.whl (138 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.0/139.0 KB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: sagemaker, protobuf3-to-dict\n",
      "  Building wheel for sagemaker (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sagemaker: filename=sagemaker-2.87.0-py2.py3-none-any.whl size=721756 sha256=45469cd2d32c784bebf590c4a5024dd0452d6e7bd2e91bd50bfa28ba6c09ffba\n",
      "  Stored in directory: /Users/lokravi/Library/Caches/pip/wheels/a7/ff/a3/9cf63288d45e488e1e9b34afdde6db1c39506cea8628153889\n",
      "  Building wheel for protobuf3-to-dict (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for protobuf3-to-dict: filename=protobuf3_to_dict-0.1.5-py3-none-any.whl size=4029 sha256=ae297746ab14d72b3152d142541972c2381a6acb157199400e9b49184a59e3b2\n",
      "  Stored in directory: /Users/lokravi/Library/Caches/pip/wheels/21/bf/76/90dd7b8d0598c7642532062ddff00ecef07df873c36396488c\n",
      "Successfully built sagemaker protobuf3-to-dict\n",
      "Installing collected packages: pytz, pox, zipp, urllib3, smdebug_rulesconfig, six, pyparsing, protobuf, numpy, jmespath, dill, attrs, python-dateutil, protobuf3-to-dict, ppft, packaging, multiprocess, importlib-metadata, google-pasta, pathos, pandas, botocore, s3transfer, boto3, sagemaker\n",
      "  Attempting uninstall: pytz\n",
      "    Found existing installation: pytz 2021.3\n",
      "    Uninstalling pytz-2021.3:\n",
      "      Successfully uninstalled pytz-2021.3\n",
      "\u001b[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m  Attempting uninstall: pox\n",
      "    Found existing installation: pox 0.3.0\n",
      "    Uninstalling pox-0.3.0:\n",
      "      Successfully uninstalled pox-0.3.0\n",
      "\u001b[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m  Attempting uninstall: zipp\n",
      "    Found existing installation: zipp 3.7.0\n",
      "    Uninstalling zipp-3.7.0:\n",
      "      Successfully uninstalled zipp-3.7.0\n",
      "\u001b[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.26.8\n",
      "    Uninstalling urllib3-1.26.8:\n",
      "      Successfully uninstalled urllib3-1.26.8\n",
      "\u001b[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m  Attempting uninstall: smdebug_rulesconfig\n",
      "    Found existing installation: smdebug-rulesconfig 1.0.1\n",
      "    Uninstalling smdebug-rulesconfig-1.0.1:\n",
      "      Successfully uninstalled smdebug-rulesconfig-1.0.1\n",
      "\u001b[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m  Attempting uninstall: six\n",
      "    Found existing installation: six 1.16.0\n",
      "    Uninstalling six-1.16.0:\n",
      "      Successfully uninstalled six-1.16.0\n",
      "\u001b[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m  Attempting uninstall: pyparsing\n",
      "    Found existing installation: pyparsing 3.0.6\n",
      "    Uninstalling pyparsing-3.0.6:\n",
      "      Successfully uninstalled pyparsing-3.0.6\n",
      "\u001b[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.19.4\n",
      "    Uninstalling protobuf-3.19.4:\n",
      "      Successfully uninstalled protobuf-3.19.4\n",
      "\u001b[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.22.1\n",
      "    Uninstalling numpy-1.22.1:\n",
      "      Successfully uninstalled numpy-1.22.1\n",
      "\u001b[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m  Attempting uninstall: jmespath\n",
      "    Found existing installation: jmespath 0.10.0\n",
      "    Uninstalling jmespath-0.10.0:\n",
      "      Successfully uninstalled jmespath-0.10.0\n",
      "\u001b[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.4\n",
      "    Uninstalling dill-0.3.4:\n",
      "      Successfully uninstalled dill-0.3.4\n",
      "\u001b[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m  Attempting uninstall: attrs\n",
      "    Found existing installation: attrs 21.2.0\n",
      "    Uninstalling attrs-21.2.0:\n",
      "      Successfully uninstalled attrs-21.2.0\n",
      "\u001b[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m  Attempting uninstall: python-dateutil\n",
      "    Found existing installation: python-dateutil 2.8.2\n",
      "    Uninstalling python-dateutil-2.8.2:\n",
      "      Successfully uninstalled python-dateutil-2.8.2\n",
      "\u001b[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m  Attempting uninstall: protobuf3-to-dict\n",
      "    Found existing installation: protobuf3-to-dict 0.1.5\n",
      "    Uninstalling protobuf3-to-dict-0.1.5:\n",
      "      Successfully uninstalled protobuf3-to-dict-0.1.5\n",
      "\u001b[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m  Attempting uninstall: ppft\n",
      "    Found existing installation: ppft 1.6.6.4\n",
      "    Uninstalling ppft-1.6.6.4:\n",
      "      Successfully uninstalled ppft-1.6.6.4\n",
      "\u001b[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 21.3\n",
      "    Uninstalling packaging-21.3:\n",
      "      Successfully uninstalled packaging-21.3\n",
      "\u001b[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m  Attempting uninstall: multiprocess\n",
      "    Found existing installation: multiprocess 0.70.12.2\n",
      "    Uninstalling multiprocess-0.70.12.2:\n",
      "      Successfully uninstalled multiprocess-0.70.12.2\n",
      "\u001b[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m  Attempting uninstall: importlib-metadata\n",
      "    Found existing installation: importlib-metadata 4.10.1\n",
      "    Uninstalling importlib-metadata-4.10.1:\n",
      "      Successfully uninstalled importlib-metadata-4.10.1\n",
      "\u001b[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m  Attempting uninstall: google-pasta\n",
      "    Found existing installation: google-pasta 0.2.0\n",
      "    Uninstalling google-pasta-0.2.0:\n",
      "      Successfully uninstalled google-pasta-0.2.0\n",
      "\u001b[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m  Attempting uninstall: pathos\n",
      "    Found existing installation: pathos 0.2.8\n",
      "    Uninstalling pathos-0.2.8:\n",
      "      Successfully uninstalled pathos-0.2.8\n",
      "\u001b[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.2.0\n",
      "    Uninstalling pandas-1.2.0:\n",
      "      Successfully uninstalled pandas-1.2.0\n",
      "\u001b[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.23.52\n",
      "    Uninstalling botocore-1.23.52:\n",
      "      Successfully uninstalled botocore-1.23.52\n",
      "\u001b[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m  Attempting uninstall: s3transfer\n",
      "    Found existing installation: s3transfer 0.5.0\n",
      "    Uninstalling s3transfer-0.5.0:\n",
      "      Successfully uninstalled s3transfer-0.5.0\n",
      "\u001b[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m  Attempting uninstall: boto3\n",
      "    Found existing installation: boto3 1.20.48\n",
      "    Uninstalling boto3-1.20.48:\n",
      "      Successfully uninstalled boto3-1.20.48\n",
      "\u001b[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m  Attempting uninstall: sagemaker\n",
      "    Found existing installation: sagemaker 2.81.1\n",
      "    Uninstalling sagemaker-2.81.1:\n",
      "      Successfully uninstalled sagemaker-2.81.1\n",
      "\u001b[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "awscli 1.22.52 requires botocore==1.23.52, but you have botocore 1.25.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed attrs-20.3.0 boto3-1.22.1 botocore-1.25.1 dill-0.3.4 google-pasta-0.2.0 importlib-metadata-4.11.3 jmespath-1.0.0 multiprocess-0.70.12.2 numpy-1.22.3 packaging-21.3 pandas-1.4.2 pathos-0.2.8 pox-0.3.0 ppft-1.6.6.4 protobuf-3.20.1 protobuf3-to-dict-0.1.5 pyparsing-3.0.8 python-dateutil-2.8.2 pytz-2022.1 s3transfer-0.5.2 sagemaker-2.87.0 six-1.16.0 smdebug_rulesconfig-1.0.1 urllib3-1.26.9 zipp-3.8.0\n"
     ]
    }
   ],
   "source": [
    "!pip install --force-reinstall sagemaker==2.87.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c266b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: transformers==4.17.0 in /usr/local/lib/python3.9/site-packages (4.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.9/site-packages (from transformers==4.17.0) (0.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/site-packages (from transformers==4.17.0) (4.62.3)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.9/site-packages (from transformers==4.17.0) (0.0.49)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/site-packages (from transformers==4.17.0) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/site-packages (from transformers==4.17.0) (5.4.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,>=0.11.1 in /usr/local/lib/python3.9/site-packages (from transformers==4.17.0) (0.11.6)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/site-packages (from transformers==4.17.0) (1.22.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/site-packages (from transformers==4.17.0) (3.6.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/site-packages (from transformers==4.17.0) (2.27.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/site-packages (from transformers==4.17.0) (2021.11.10)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.17.0) (4.0.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.9/site-packages (from packaging>=20.0->transformers==4.17.0) (3.0.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/site-packages (from requests->transformers==4.17.0) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/site-packages (from requests->transformers==4.17.0) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/site-packages (from requests->transformers==4.17.0) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/site-packages (from requests->transformers==4.17.0) (2.0.10)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.9/site-packages (from sacremoses->transformers==4.17.0) (1.16.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.9/site-packages (from sacremoses->transformers==4.17.0) (8.0.3)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.9/site-packages (from sacremoses->transformers==4.17.0) (1.1.0)\n",
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install transformers==4.17.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3651b473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker: 2.87.0\n",
      "transformers: 4.17.0\n"
     ]
    }
   ],
   "source": [
    "import botocore\n",
    "import boto3\n",
    "import sagemaker\n",
    "import transformers\n",
    "import pandas as pd\n",
    "\n",
    "print(f\"sagemaker: {sagemaker.__version__}\")\n",
    "print(f\"transformers: {transformers.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7598fb",
   "metadata": {},
   "source": [
    "### Development environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5efa7ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::875423407011:role/Admin\n",
      "sagemaker bucket: sagemaker-us-west-2-875423407011\n",
      "sagemaker session region: us-west-2\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "# SageMaker session bucket -> used for uploading data, models and logs\n",
    "# SageMaker will automatically create this bucket if it does not exist\n",
    "sagemaker_session_bucket = None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6aa609c",
   "metadata": {},
   "source": [
    "## Finding max batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326fa565",
   "metadata": {},
   "source": [
    "### Model and instance type specifications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85d91c1",
   "metadata": {},
   "source": [
    "This notebook uses Hugging Face training script to demonstrate how to find the max batch size that can fit in memory, if you're using a customized training script, please update `find_max_batch_size.py` script and `hyperparameters` accordingly. Below, we specify the model we would like to find the max batch size for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0c5e04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "LANGUAGE_MODELING_LOSS = \"clm\"\n",
    "\n",
    "MODEL_NAME = \"gpt2\"\n",
    "TOKENIZER_NAME = \"gpt2\"\n",
    "MODEL_CONFIG = \"model_name_or_path\"\n",
    "\n",
    "INSTANCE_TYPE = \"ml.p3.8xlarge\"\n",
    "\n",
    "# hyperparameters are passed to the training entrypoint as arguments\n",
    "hyperparameters = {\n",
    "    \"training_script\": f\"run_{LANGUAGE_MODELING_LOSS}.py\",\n",
    "    MODEL_CONFIG: MODEL_NAME,\n",
    "    \"tokenizer_name\": TOKENIZER_NAME,\n",
    "    \"fp16\": True,\n",
    "    \"sequence_len\": 512,\n",
    "    \"per_device_train_batch_size_min\": 1,\n",
    "    \"per_device_train_batch_size_max\": 128,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd53820",
   "metadata": {},
   "source": [
    "### Finding max batch size for SageMaker Training Compiler with Hugging Face and TensorFlow\n",
    "\n",
    "We use the wrapper script below to find the maximum batch size. The wrapper assumes that Cuda will throw an Out Of Memory error if the batch size is too high. We then use binary seach to find the maximum batch size that does not cause the training script to fail. In the interest of time, we only check the first 10 training steps for memory overflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e73c7944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m, \u001b[04m\u001b[36msubprocess\u001b[39;49;00m, \u001b[04m\u001b[36mtime\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\r\n",
      "\r\n",
      "    parser = argparse.ArgumentParser()\r\n",
      "    \u001b[37m# please update parameters if using a customized training script\u001b[39;49;00m\r\n",
      "    \u001b[37m# model configs\u001b[39;49;00m\r\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--language_modeling_loss\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m\"\u001b[39;49;00m\u001b[33mclm\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mselect either use training script run_mlm or run_clm\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--model_name_or_path\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m\"\u001b[39;49;00m\u001b[33mgpt2\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mHF model name\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r",
      "\r\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--tokenizer_name\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m\"\u001b[39;49;00m\u001b[33mgpt2\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mtokenizer name\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--sequence_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m\"\u001b[39;49;00m\u001b[33m512\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33msequence length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--fp16\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, action=\u001b[33m'\u001b[39;49;00m\u001b[33mstore_true\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mwhether to train in amp mode\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[37m# batch size config\u001b[39;49;00m\r\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--per_device_train_batch_size_min\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m1\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mminimum batch size to try\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--per_device_train_batch_size_max\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m256\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mmaximum batch size to try\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    args, rem_args = parser.parse_known_args()\r\n",
      "\r\n",
      "    \u001b[37m# Causal Language Modeling (run_clm) or Masked Language Modeling (run_mlm)\u001b[39;49;00m\r\n",
      "    training_command = \u001b[33m\"\u001b[39;49;00m\u001b[33mpython \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "    \u001b[34mif\u001b[39;49;00m args.language_modeling_loss == \u001b[33m\"\u001b[39;49;00m\u001b[33mmlm\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\r\n",
      "        training_command += \u001b[33m\"\u001b[39;49;00m\u001b[33m./run_mlm.py \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "    \u001b[34melse\u001b[39;49;00m:\r\n",
      "        training_command += \u001b[33m\"\u001b[39;49;00m\u001b[33m./run_clm.py \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "    training_command += \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m--model_name_or_path \u001b[39;49;00m\u001b[33m{\u001b[39;49;00margs.model_name_or_path\u001b[33m}\u001b[39;49;00m\u001b[33m \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "    training_command += \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m--max_seq_length \u001b[39;49;00m\u001b[33m{\u001b[39;49;00margs.sequence_len\u001b[33m}\u001b[39;49;00m\u001b[33m \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m args.language_modeling_loss == \u001b[33m\"\u001b[39;49;00m\u001b[33mmlm\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34melse\u001b[39;49;00m (\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m--block_size \u001b[39;49;00m\u001b[33m{\u001b[39;49;00margs.sequence_len\u001b[33m}\u001b[39;49;00m\u001b[33m \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    training_command += \u001b[33m\"\u001b[39;49;00m\u001b[33m--dataset_name glue \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "    training_command += \u001b[33m\"\u001b[39;49;00m\u001b[33m--dataset_config_name sst2 \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "    training_command += \u001b[33m\"\u001b[39;49;00m\u001b[33m--do_train \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "    training_command += \u001b[33m\"\u001b[39;49;00m\u001b[33m--num_train_epochs 1 \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "    training_command += \u001b[33m\"\u001b[39;49;00m\u001b[33m--max_steps 1000 \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "    training_command += \u001b[33m\"\u001b[39;49;00m\u001b[33m--save_strategy no \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "    training_command += \u001b[33m\"\u001b[39;49;00m\u001b[33m--logging_strategy no \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "    training_command += \u001b[33m\"\u001b[39;49;00m\u001b[33m--output_dir /tmp/test \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "    training_command += \u001b[33m\"\u001b[39;49;00m\u001b[33m--overwrite_output_dir \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "    \u001b[34mif\u001b[39;49;00m args.fp16:\r\n",
      "        training_command += \u001b[33m\"\u001b[39;49;00m\u001b[33m--fp16 \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "    \u001b[37m# find max batch size between per_device_train_batch_size_min and per_device_train_batch_size_max\u001b[39;49;00m\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mTuning Command: \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, training_command)\r\n",
      "    \u001b[34massert\u001b[39;49;00m args.per_device_train_batch_size_min >= \u001b[34m1\u001b[39;49;00m \u001b[35mand\u001b[39;49;00m args.per_device_train_batch_size_min <= args.per_device_train_batch_size_max\r\n",
      "    batch_result = \u001b[34m0\u001b[39;49;00m\r\n",
      "    low, high = args.per_device_train_batch_size_min, args.per_device_train_batch_size_max\r\n",
      "    tic, i = time.perf_counter(), \u001b[34m0\u001b[39;49;00m\r\n",
      "    \u001b[34mwhile\u001b[39;49;00m low <= high:\r\n",
      "        batch_to_try, i = (low + high) // \u001b[34m2\u001b[39;49;00m, i + \u001b[34m1\u001b[39;49;00m\r\n",
      "        log_info = \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mmodel: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00margs.model_name_or_path\u001b[33m}\u001b[39;49;00m\u001b[33m trying batch_size: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mbatch_to_try\u001b[33m}\u001b[39;49;00m\u001b[33m \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "        \u001b[34mtry\u001b[39;49;00m:\r\n",
      "            \u001b[37m# please update batch_size parameter naming if using a customized training script\u001b[39;49;00m\r\n",
      "            training_command_batch = training_command + \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m--per_device_train_batch_size \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mbatch_to_try\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "            subprocess.check_output(training_command_batch, shell=\u001b[34mTrue\u001b[39;49;00m)\r\n",
      "            batch_result, low = batch_to_try, batch_to_try + \u001b[34m1\u001b[39;49;00m\r\n",
      "            log_info += \u001b[33m\"\u001b[39;49;00m\u001b[33msucceed\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "        \u001b[34mexcept\u001b[39;49;00m subprocess.CalledProcessError \u001b[34mas\u001b[39;49;00m exc:\r\n",
      "            high = batch_to_try - \u001b[34m1\u001b[39;49;00m\r\n",
      "            log_info += \u001b[33m\"\u001b[39;49;00m\u001b[33mfailed\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "        \u001b[36mprint\u001b[39;49;00m(log_info)\r\n",
      "    toc = time.perf_counter()\r\n",
      "\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mTotal max batch found in \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mtoc - tic\u001b[33m:\u001b[39;49;00m\u001b[33m0.4f\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m\u001b[33m seconds, \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mi\u001b[33m}\u001b[39;49;00m\u001b[33m iterations\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m[result]: model: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00margs.model_name_or_path\u001b[33m}\u001b[39;49;00m\u001b[33m, max_batch_size between \u001b[39;49;00m\u001b[33m{\u001b[39;49;00margs.per_device_train_batch_size_min\u001b[33m}\u001b[39;49;00m\u001b[33m and \u001b[39;49;00m\u001b[33m{\u001b[39;49;00margs.per_device_train_batch_size_max\u001b[33m}\u001b[39;49;00m\u001b[33m is \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mbatch_result\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n"
     ]
    }
   ],
   "source": [
    "# This prints the training script for reference\n",
    "# The script iteratively tests different batch sizes\n",
    "!pygmentize ./scripts/find_max_batch_size.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "temp-e6aa609c",
   "metadata": {},
   "source": [
    "### Configure a SageMaker Hugging Face estimator with the SageMaker Training Compiler configuration and the script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98587d1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'huggingface-tensorflow-trcomp-training-2022-04-27-17-15-34-114'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagemaker.huggingface import HuggingFace, TrainingCompilerConfig\n",
    "\n",
    "# configure the training job\n",
    "optimized_estimator = HuggingFace(\n",
    "    entry_point=\"find_max_batch_size.py\",  # Wrapper around training script that finds the maximum batch size\n",
    "    compiler_config=TrainingCompilerConfig(),  # We are enabling SageMaker Training Compiler here !\n",
    "    source_dir=\"./scripts\",\n",
    "    instance_type=INSTANCE_TYPE,\n",
    "    instance_count=1,\n",
    "    role=role,\n",
    "    volume_size=100,\n",
    "    py_version=\"py38\",\n",
    "    transformers_version=\"4.17.0\",\n",
    "    tensorflow_version=\"2.6.3\",\n",
    "    hyperparameters=hyperparameters,\n",
    "    disable_profiler=True,  # Disabling SageMaker Profiler to avoid overheads during benchmarking\n",
    "    debugger_hook_config=False,  # Disabling SageMaker Debugger to avoid overheads during benchmarking\n",
    ")\n",
    "\n",
    "# start the training job\n",
    "optimized_estimator.fit(wait=False)\n",
    "optimized_estimator.latest_training_job.name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbecffb3",
   "metadata": {},
   "source": [
    "### Wait for the training job to complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dcd5f899",
   "metadata": {},
   "outputs": [],
   "source": [
    "waiter = optimized_estimator.sagemaker_session.sagemaker_client.get_waiter(\n",
    "    \"training_job_completed_or_stopped\"\n",
    ")\n",
    "waiter.wait(TrainingJobName=optimized_estimator.latest_training_job.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5aa2ff6",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bca81ba",
   "metadata": {},
   "source": [
    "### Load logs for training jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8454da62",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture optimized\n",
    "\n",
    "# access the logs of the optimized training job\n",
    "optimized_estimator.sagemaker_session.logs_for_job(optimized_estimator.latest_training_job.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ffc316e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mTotal max batch found in 25.9129 seconds, 7 iterations\u001b[0m\n",
      "\u001b[34m[result]: model: gpt2, max_batch_size between 1 and 128 is 0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Print the max batch size below\n",
    "\n",
    "for line in optimized.stdout.split(\"\\n\"):\n",
    "    if \"result\" in line and \"max_batch_size\" in line or \"Total max batch\" in line:\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71ea509",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "\n",
    "Stop all training jobs launched if the jobs are still running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae4ecab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "sm = boto3.client(\"sagemaker\")\n",
    "\n",
    "\n",
    "def stop_training_job(name):\n",
    "    status = sm.describe_training_job(TrainingJobName=name)[\"TrainingJobStatus\"]\n",
    "    if status == \"InProgress\":\n",
    "        sm.stop_training_job(TrainingJobName=name)\n",
    "\n",
    "\n",
    "stop_training_job(optimized_estimator.latest_training_job.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e33f76",
   "metadata": {},
   "source": [
    "Also, to find instructions on cleaning up resources, see [Clean Up](https://docs.aws.amazon.com/sagemaker/latest/dg/ex1-cleanup.html) in the *Amazon SageMaker Developer Guide*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528c5820",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "SageMaker Training Compiler improves the efficiency of your training job by typically decreasing the memory footprint of the job. In this notebook, you found the largest `batch_size` that can fit in memory with Training Compiler's optimizations. Increasing the `batch_size` can decrease the time needed to train a model, reducing cost and enabling faster iteration.\n",
    "\n",
    "Remember that learning rate should be adjusted when `batch_size` is changed to minimize difference in convergence behavior during training. For more information on how learning rate is connected to batch size, see [Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour](https://arxiv.org/abs/1706.02677)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2048393",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
